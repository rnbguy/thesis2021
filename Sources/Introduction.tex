%!TEX root = ../Thesis.tex

% \section{Distributed Systems}

%% What is distributed system. Why it is necessary. Where it is used.

As \internet{} grows to be cheaper and faster, distributed systems and applications are also becoming popular over past decades.
Today they are the backbone of almost any online application like banking, shopping, ticketing, social networking \etc{}.
As the popularity of these softwares increases, it is very important that these systems are reliable and secure.

%% How does it work.

Distributed systems are usually deployed over multiple locations connected through \internet{}.
There are multiple good reasons why an online application would need that.
The users of the application may require data availability and fast response.
Or, if some location goes offline, the other locations can still serve the users.
Usually each distributed \emph{nodes} maintains a replica of the global data.
A user connects to the node \emph{nearest} to them and that node serves their requests.
This way the distributed system reduces responses time and distributes the workload to multiple nodes. 

Distributed systems are the solution to a fast, concurrent, scalable online applications. But it actually offers a trade off.
As we allow concurrent operations at multiple nodes, they still need to synchronize among them and maintain a meaningful or \emph{consistent} global data.
A global lock defeats the whole purpose of the distributed system, because taking a lock over a multiple distributed nodes means more communication over internet and slow response time.

Over the past few years, the community came up with many implementations \emph{weakly-consistent} distributed system.
They throw away strong consistent guarantees of the global data in favour of a scalable system.
This is almost always alright. Because in most cases, the distributed applications do not really require strong guarantees over the data.

But these implementations are often not correctly reasoned.
%% examples of real world problems
Many cases the guaranteed weak consistencies are explained in words and not formalized, which makes it difficult to reason about them.
Also, often these systems promise a weak consistency on paper, but implements something else in practice.
So it is very crucial, these systems are tested thoroughly against the consistency needs of an application.

This dissertation is an effort to give solutions to these problems. We first study \emph{Conflict Free Replicated Data Type} or \emph{CRDT}. CRDTs are popular data types like sets, flags, registers, but replicated and maintained at multiple nodes. Then we move on to \emph{Transactional Systems} which allows to perform a set of operations isolated from other sets of operations. In both cases we reason about their consistency guarantees and provide algorithms to test their histories against the promised weak-consistency. Lastly, we study the correctness of applications implemented over distributed systems guaranteeing weak consistency level. We implement an artificial data storage, \tool{}, based on our previous formalization of consistency levels to simulate the possible abstraction of weak scenarios such as network partition, packet drops \etc{}. These applications can use \tool{} as a drop-in replacement to their backend databases to inject faults without any external control over network.

%%%%%%%%%
% CRDT
%%%%%%%%

\section{Conflict Free Replicated Data Types}

Replication is used in any distributed system. But to maintain a reliable replicated data, one has to maintain some level of consistency through clever synchronization techniques. The standard \emph{strong consistency} approach is to find a linearization of the set of all global operations. But as CAP theorem shows, achieving strong consistency is not possible while maintaining availability and network partition.

When network delays are big and network partition is an issue, \emph{eventual consistency} promises better performance and availability. An update gets executed at some replica but it may be sent to other replicas later as a delayed synchronization. Although updates eventually reaches to all replicas, these updates may be seen concurrently, even in different orders, by different replicas. If these concurrent updates are \emph{conflicting}, they need to be resolved. This is not trivial because the resolved outcome need to be the \emph{same} in all replicas such that the replicas do not diverge.

\emph{Conflict-free replicated data types} (CRDTs)~\cite{DBLP:conf/sss/ShapiroPBZ11} efficiently resolve the effects of concurrent updates to replicated data. In figure \ref{fig:crdt:intro}, we demonstrate a CRDT, specifically an \emph{OR-Set}.
OR-Set implements a distributed Set with natural \texttt{add(\_)}, \texttt{remove(\_)}, \texttt{contain(\_)} operations. \texttt{add(\_)} and \texttt{remove(\_)} are the only update operations. Two updates are in conflict if they are trying to insert or remove a same element. As its CRDT based conflict-resolve technique, an \texttt{add(\_)} operation will always win among multiple conflicting updates.
In figure \ref{fig:crdt:intro}, \texttt{read()} can seen as performing \texttt{contain(\_)} on $a$ and $b$. In each replica $r_i$, everything seems fine when only \texttt{add(\_)} is happening. In $r_1$, after adding $a$ and $b$ and getting the update from $r_2$, the internal set contains both $a$ and $b$. After performing \texttt{remove(b)}, $b$ gets remove from the set. Now, the update of \texttt{add(b)} from $r_2$ reaches $r_1$. Since last \texttt{remove(b)} on $r_1$ and \texttt{add(b)} from $r_2$ are concurrent, meaning they are not causally related, they are in conflict. The OR-Set resolves them to the \texttt{add(b)}, so $b$ gets added in the set. Then the \texttt{remove(a)} from $r_2$ reaches to $r_1$. Like before, the \texttt{add(a)} on $r_1$ and \texttt{remove(a)} from $r_2$ are in conflict. The OR-Set resolves them to the \texttt{add(a)} and ignores the \texttt{remove(b)} from $r_2$. So $a$ and $b$ both persists in the set. The same can be explained for $r_2$'s observation. It can be shown, this behavior is not linearizable. This behavior is not possible when strong guarantees are promised.
Similarly, \emph{And-Set} is a CRDT in which \texttt{remove(\_)} always wins among multiple conflicting updates.

Recent distributed systems have introduced variations of familiar abstract data types (ADTs) like counters, registers, flags, and sets, that provide high availability and partition tolerance. Naturally they weaken consistency guarantees to achieve availability and partition-tolerance, and various notions of \emph{weak consistency} capture such guarantees~\cite{DBLP:conf/pdis/TerryDPSTW94, DBLP:conf/sosp/TerryTPDSH95, DBLP:conf/popl/MansonPA05, DBLP:journals/ftpl/Burckhardt14, DBLP:conf/popl/BurckhardtGYZ14}. Although, there are not much work to verify these implementations. We try to solve the problem using runtime-testing.


\subsection{State of the Art}

The consistency models to specify these replicated data types.
Many have considered consistency models applicable to CRDTs, including causal consistency~\cite{DBLP:journals/cacm/Lamport78}, sequential consistency~\cite{DBLP:journals/tc/Lamport79}, linearizability~\cite{DBLP:journals/toplas/HerlihyW90}, session consistency~\cite{DBLP:conf/pdis/TerryDPSTW94}, eventual consistency~\cite{DBLP:conf/sosp/TerryTPDSH95}, and happens-before consistency~\cite{DBLP:conf/popl/MansonPA05}. Burckhardt et al.~\cite{DBLP:journals/ftpl/Burckhardt14, DBLP:conf/popl/BurckhardtGYZ14} propose a unifying framework to formalize these models. Many have also studied the complexity of verifying data-type agnostic notions of consistency, including serializability, sequential consistency and linearizability~\cite{DBLP:journals/jacm/Papadimitriou79b, DBLP:journals/siamcomp/GibbonsK97, DBLP:journals/iandc/AlurMP00, DBLP:conf/spaa/BinghamCH03, DBLP:conf/cav/FarzanM08, DBLP:conf/esop/BouajjaniEEH13, DBLP:conf/netys/Hamza15}, as well as causal consistency~\cite{DBLP:conf/popl/BouajjaniEGH17}.
% Our definition of the replicated LWW register corresponds to the notion of causal convergence in~\cite{DBLP:conf/popl/BouajjaniEGH17}. This work studies the complexity of the admissibility problem for the replicated LWW register. It shows that this problem is NP-complete in general and polynomial time when each value is written only once. 
% Our NP-completeness result is stronger since it assumes a fixed number of replicas, and our algorithm for the case of unique values is more general and can be applied uniformly to MVR and RGA. 
Bouajjani et al.~\cite{DBLP:journals/iandc/BouajjaniEEH18, DBLP:journals/pacmpl/EmmiE18} consider the complexity for individual linearizable collection types. Others have developed effective consistency checking algorithms for sequential consistency~\cite{DBLP:conf/cav/HenzingerQR99a, DBLP:journals/tpds/Qadeer03, DBLP:conf/cav/BinghamCHQZ04, DBLP:conf/pldi/BurckhardtAM07}, serializability~\cite{DBLP:conf/fmcad/0002OPTZ07, DBLP:conf/cav/FarzanM08, DBLP:conf/pldi/GuerraouiHJS08, DBLP:conf/pldi/EmmiMM10}, linearizability~\cite{DBLP:journals/jpdc/WingG93, DBLP:conf/pldi/BurckhardtDMT10, DBLP:conf/pldi/EmmiEH15, DBLP:journals/concurrency/Lowe17}, and even weaker notions like eventual consistency~\cite{DBLP:conf/popl/BouajjaniEH14} and sequential happens-before consistency~\cite{DBLP:conf/cav/EmmiE18, DBLP:journals/pacmpl/EmmiE19}.


\subsection{Contribution}

In this work we study the tractability of CRDT consistency checking. In particular, we consider \emph{runtime verification}: deciding whether a given execution of a CRDT is consistent with its ADT specification. This problem is particularly relevant as distributed-system testing tools like Jepsen~\cite{MISC:Jepsen} are appearing; without efficient, general consistency-checking algorithms, such tools could be limited to specialized classes of errors like node crashes.

Our study proceeds in three parts.
First, to precisely characterize the consistency of various CRDTs, and facilitate symbolic reasoning, we develop novel logical characterizations to capture their guarantees.
Second, we demonstrate the intractability of several replicated data types by reduction from propositional satisfiability (SAT) problems.
Third, we develop tractable consistency-checking algorithms for individual data types and special cases: replicated growing arrays; multi-value and last-writer-wins registers,Â when each value is written only once; counters, when replicas are bounded; and sets and flags, when their sizes are also bounded.


\begin{figure}
% \resizebox{\textwidth}{!}{
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,semithick, transform shape]

\draw (0, 0) -- (10, 0);
\draw (0, 2.5) -- (10, 2.5);

\node[label=left:{$r_2$}] at (0, 0) {};
\node[label=left:{$r_1$}] at (0, 2.5) {};

\node[draw,circle,fill=black,scale=0.3,label=below:{\texttt{add(a)}}](r21) at (.5, 0) {};
\node[draw,circle,fill=black,scale=0.3,label=below:{\texttt{add(b)}}](r22) at (2, 0) {};
\node[draw,circle,fill=black,scale=0.3,label=below:{\texttt{remove(a)}}](r23) at (5, 0) {};
\node[draw,circle,fill=black,scale=0.3,label=below:{\texttt{read()=\{a,b\}}}] at (9, 0) {};

\node[draw,circle,fill=black,scale=0.3,label=above:{\texttt{add(b)}}](r11) at (.5, 2.5) {};
\node[draw,circle,fill=black,scale=0.3,label=above:{\texttt{add(a)}}](r12) at (2, 2.5) {};
\node[draw,circle,fill=black,scale=0.3,label=above:{\texttt{remove(b)}}](r13) at (5, 2.5) {};
\node[draw,circle,fill=black,scale=0.3,label=above:{\texttt{read()=\{a,b\}}}] at (9, 2.5) {};

\node[label=above:{\texttt{\{a\}}}] at (1, 0) {};
\node[label=above:{\texttt{\{a,b\}}}] at (2.5, 0) {};
\node[label=above:{\texttt{\{a,b\}}}] at (4, 0) {};
\node[label=above:{\texttt{\{b\}}}] at (5.5, 0) {};
\node[label=above:{\texttt{\{a,b\}}}] at (7, 0) {};
\node[label=above:{\texttt{\{a,b\}}}] at (8.5, 0) {};

\node[label=below:{\texttt{\{b\}}}] at (1, 2.5) {};
\node[label=below:{\texttt{\{a,b\}}}] at (2.5, 2.5) {};
\node[label=below:{\texttt{\{a,b\}}}] at (4, 2.5) {};
\node[label=below:{\texttt{\{a\}}}] at (5.5, 2.5) {};
\node[label=below:{\texttt{\{a,b\}}}] at (7, 2.5) {};
\node[label=below:{\texttt{\{a,b\}}}] at (8.5, 2.5) {};

\draw (r11) edge[out=-90,in=90] (3.5, 0);
\draw (r12) edge[out=-90,in=90] (6.5, 0);
\draw (r13) edge[out=-90,in=90] (8, 0);

\draw (r21) edge[out=90,in=-90] (3.5, 2.5);
\draw (r22) edge[out=90,in=-90] (6.5, 2.5);
\draw (r23) edge[out=90,in=-90] (8, 2.5);

\end{tikzpicture}  
% }
\caption{A non-linearizable OR-Set execution}
\label{fig:crdt:intro}
\end{figure}


%%%%%%%%%%
% Transaction
%%%%%%%%%%

\section{Transactional Systems}


% \texttt{Payment(k)} \\
% \texttt{x = read_balance()} \\
% \texttt{x = read_balance()} \\ 
% \texttt{update_balance(x - k)} \\

\begin{figure}
    \centering
    \begin{minipage}{.45\textwidth}
        \resizebox{\textwidth}{!}{
         \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
           semithick, transform shape]
          \node[draw,dashed,rounded corners=2mm] (t1) at (0, 0) {\begin{tabular}{l}
            \texttt{Payment(k):} \\
            \texttt{  x = read\_balance()} \\
            \texttt{  ...} \\ 
            \texttt{  update\_balance(x - k)} \end{tabular}};
            \node[draw,dashed,rounded corners=2mm] (t2) at (5, 0) {\begin{tabular}{l}
              \texttt{Payment(k):} \\
              \texttt{  x = read\_balance()} \\
              \texttt{  ...} \\ 
              \texttt{  update\_balance(x - k)} \end{tabular}};
          \path (2, 0.2) edge (3, 0.2);
          \path (3, 0.2) edge (2, -0.7);
          \path (2, -0.7) edge (3, -0.7);
        %   \path (t1) edge[red, bend left] node {$\co$} (t2);
        %   \path (t4_2) edge node {$\po$} (t4_3);
        %   \path (t2) edge[blue, bend left] node {$\co$} (t1);
         \end{tikzpicture}  
        }
        \caption{Non-transactional execution}
        \label{fig:txn:intro:1}
       \end{minipage}
       \hspace{.5cm}
       \begin{minipage}{.45\textwidth}
        \resizebox{\textwidth}{!}{
         \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
           semithick, transform shape]
          \node[draw, rounded corners=2mm] (t1) at (0, 0) {\begin{tabular}{l}
            \texttt{Payment(k):} \\
            \texttt{  x = read\_balance()} \\
            \texttt{  ...} \\ 
            \texttt{  update\_balance(x - k)} \end{tabular}};
            \node[draw, rounded corners=2mm] (t2) at (5, 0) {\begin{tabular}{l}
              \texttt{Payment(k):} \\
              \texttt{  x = read\_balance()} \\
              \texttt{  ...} \\
              \texttt{  update\_balance(x - k) } \\
              \texttt{  // error -> rollback } \end{tabular}};
        %   \path (t4_1) edge node {$\po$} (t4_2);
        %   \path (t1) edge[red, bend left] node {$\co$} (t2);
        %   \path (t4_2) edge node {$\po$} (t4_3);
        %   \path (t2) edge[blue, bend left] node {$\co$} (t1);
         \end{tikzpicture}  
        }
        \caption{Transactional execution}
        \label{fig:txn:intro:2}
    \end{minipage}
% \caption{Behaviors in non-transactional and transactional system}
\end{figure}

Transactions simplify concurrent programming by enabling multiple computations on shared data that are isolated from other concurrent computations and resilient to failures.
In figure \ref{fig:txn:intro:1}, we have two distributed nodes tries to perform a simple \textsf{Payment} code. If we allow the internal read and write operations to be interleaved, we can have a scenario where the reads happens first and then the write. This would allow a user to pay 200 euros spending only 100 euros from their balance. So transactions are introduced to keep such critical code blocks to be performed in isolation. In figure \ref{fig:txn:intro:1}, the same code is run under transactions and one of them throws error. Modern databases provide transactions in various forms corresponding to different tradeoffs between consistency and availability. The strongest level of consistency is achieved with \emph{serializable} transactions~\cite{DBLP:journals/jacm/Papadimitriou79b} whose outcome in concurrent executions is the same as if the transactions were executed atomically in some order. Unfortunately, serializability carries a significant penalty on the availability of the system assuming, for instance, that the database is accessed over a network that can suffer from partitions or failures. For this reason, modern databases often provide weaker guarantees about transactions, formalized by weak consistency models, e.g., causal consistency~\cite{DBLP:journals/cacm/Lamport78} and snapshot isolation~\cite{DBLP:conf/sigmod/BerensonBGMOO95}.

Implementations of large-scale databases providing transactions are difficult to build and test. For instance, distributed (replicated) databases must account for partial failures, where some components or the network can fail and produce incomplete results. Ensuring fault-tolerance relies on intricate protocols that are difficult to design and reason about. The black-box testing framework Jepsen~\cite{jepsen} found a remarkably large number of subtle problems in many production distributed databases. \footnote{https://www.infoq.com/presentations/partitioning-comparison}.

\subsection{State of the Art}

Testing a transactional database raises two issues. First, deriving a suitable set of testing scenarios, e.g., faults to inject into the system and the set of transactions to be executed, and second, deriving efficient algorithms for checking whether a given execution satisfies the considered consistency model. The Jepsen framework aims to address the first issue by using randomization, 
%shows that the first issue can be solved using randomization, 
e.g., introducing faults at random and choosing the operations in a transaction randomly. The effectiveness of this approach has been proved formally in recent work~\cite{DBLP:journals/pacmpl/OzkanMNBW18}. The second issue is, however, largely unexplored. Jepsen checks consistency in a rather ad-hoc way, focusing on specific classes of violations to a given consistency model, e.g., dirty reads (reading values from aborted transactions). This problem is challenging because the consistency specifications are non-trivial and they cannot be checked using, for instance, standard local assertions added to the client's code. 

Besides serializability, the complexity of checking correctness of an execution w.r.t. some consistency model is unknown. Checking serializability has been shown to be NP-complete~\cite{DBLP:journals/jacm/Papadimitriou79b}, and checking causal consistency in a \emph{non-transactional} context is known to be polynomial time~\cite{DBLP:conf/popl/BouajjaniEGH17}.

\subsection{Contribution}

In this work, we try to fill this gap by investigating the complexity of this problem w.r.t. several consistency models and, in the case of NP-completeness, devising algorithms that are polynomial time assuming fixed bounds for certain parameters of the input executions, e.g., the number of sessions. We consider several consistency models that are the most prevalent in practice. \emph{Read Committed} (RC)~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, \emph{Read Atomic} (RA)~\cite{DBLP:conf/concur/Cerone0G15},  \emph{Causal Consistency} (CC)~\cite{DBLP:journals/cacm/Lamport78}, \emph{Prefix Consistency} (PC)~\cite{DBLP:conf/ecoop/BurckhardtLPF15}, \emph{Snapshot Isolation} (SI)~\cite{DBLP:conf/sigmod/BerensonBGMOO95} and Serializability (SER).

%
%The only result that explores the complexity of this problem 
%
%Except for  serializability, in which case it has been shown that checking  be NP-complete~\cite{DBLP:journals/jacm/Papadimitriou79b}
%% testing, i.e., randomly choosing the faults injected into the system and the transactions to be executed is enough to reveal a
%
%
%The success of Jepsen relies on random transactions as well as randomly introduced partition faults, therefore it is solved. We tackle the second issue for a series of consistency models (Jepsen implements a test of linearizability https://github.com/jepsen-io/knossos and an ad-hoc test for causal consistency restricted to bounded executions, \url{https://github.com/jepsen-io/jepsen/blob/f345226dba1266bc37487d734a02caddf7d1d125/jepsen/src/jepsen/tests/causal.clj})
% We consider several consistency models that are the most prevalent in practice. The weakest of them, \emph{Read Committed} (RC)~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, requires that every value read in a transaction is written by a committed transaction. \emph{Read Atomic} (RA)~\cite{DBLP:conf/concur/Cerone0G15} requires that successive reads of the same variable in a transaction return the same value (also known as Repeatable Reads~\cite{DBLP:conf/sigmod/BerensonBGMOO95}), and that a transaction ``sees'' the values written by previous transactions in the same session. In general, we assume that transactions are organized in \emph{sessions}~\cite{DBLP:conf/pdis/TerryDPSTW94}, an abstraction of the sequence of transactions performed during the execution of an application.
% \emph{Causal Consistency} (CC)~\cite{DBLP:journals/cacm/Lamport78} requires that if a transaction~$\tr_1$ ``affects'' another transaction $\tr_2$, e.g., $\tr_1$ is ordered before $\tr_2$ in the same session or $\tr_2$ reads a value written by $\tr_1$, then these two transactions are observed by any other transaction in this order. \emph{Prefix Consistency} (PC)~\cite{DBLP:conf/ecoop/BurckhardtLPF15} requires that there exists a total commit order between all the transactions such that each transaction observes a prefix of this sequence. \emph{Snapshot Isolation} (SI)~\cite{DBLP:conf/sigmod/BerensonBGMOO95} further requires that two different transactions observe different prefixes if they both write to a common variable.
%Two different transactions $\tr_1$ and $\tr_2$ may observe the same prefix, which is not allowed under \emph{Snapshot Isolation} (SI)~\cite{DBLP:conf/sigmod/BerensonBGMOO95} when these two transactions write on a common variable. 
% Finally, we also provide new results concerning the problem of checking serializability (SER) that complement the known result about its NP-completeness. 

% The algorithmic issues we explore in this paper have led to a new specification framework for these consistency models that relies on the fact that the \emph{write-read} relation in an execution (also known as \emph{read-from}), relating reads with the transactions that wrote their value, can be defined effectively. The write-read relation can be extracted easily from executions where each value is written at most once (a variable can be written an arbitrary number of times). This can be easily enforced by tagging values with unique identifiers (e.g., a local counter that is incremented with every new write coupled with a client/session identifier)\footnote{This is also used in Jepsen, e.g., checking dirty reads in Galera~\cite{jepsen-galera}.}. Since practical database implementations are data-independent~\cite{DBLP:conf/popl/Wolper86}, i.e., their behavior doesn't depend on the concrete values read or written in the transactions, any potential buggy behavior can be exposed in executions where each value is written at most once. Therefore, this assumption is without loss of generality.

% Previous work~\cite{DBLP:conf/popl/BouajjaniEGH17,DBLP:conf/popl/BurckhardtGYZ14,DBLP:conf/concur/Cerone0G15} has formalized such consistency models using two auxiliary relations: a \emph{visibility} relation defining for each transaction the set of transactions it observes, and a \emph{commit order} defining the order in which transactions are committed to the ``global'' memory. An execution satisfying some consistency model is defined as the existence of a visibility relation and a commit order obeying certain axioms. In our case, the write-read relation derived from the execution plays the role of the visibility relation. This simplification allows us to state a series of axioms defining these consistency models, which have a common shape. Intuitively, they define lower bounds on the set of transactions $\tr_1$ that \emph{must} precede in commit order a transaction $\tr_2$ that is read in the execution. Besides shedding a new light on the differences between these consistency models, these axioms are essential for the algorithmic issues we investigate afterwards.

%Based on our formalization of these criteria, 
We establish that checking whether an execution satisfies RC, RA, or CC is polynomial time, while the same problem is NP-complete for PC and SI. Moreover, in the case of the NP-complete consistency models (PC, SI, SER), we show that their verification problem becomes polynomial time provided that, roughly speaking, the number of sessions in the input executions is considered to be fixed (i.e., not counted for in the input size). In more detail, we establish that checking SER reduces to a search problem in a space that has polynomial size when the number of sessions is fixed. (This algorithm applies to arbitrary executions, but its complexity would be exponential in the number of sessions in general.) Then, we show that checking PC or SI can be reduced in polynomial time to checking SER using a transformation of executions that, roughly speaking, splits each transaction in two parts: one part containing all the reads, and one part containing all the writes (SI further requires adding some additional variables in order to deal with transactions writing on a common variable).

% We extend these results even further by relying on an abstraction of executions called \emph{communication graphs}~\cite{DBLP:journals/pacmpl/ChalupaCPSV18}. Roughly speaking, the vertices of a communication graph correspond to sessions, and the edges represent the fact that two sessions access (read or write) the same variable. We show that all these criteria are polynomial-time checkable provided that the \emph{biconnected} components of the communication graph are of fixed size.

We provide an experimental evaluation of our algorithms on executions of CockroachDB~\cite{cockroach}, which claims to implement serializability~\cite{cockroach-claim} acknowledging however the possibility of anomalies, Galera~\cite{galera}, whose documentation contains contradicting claims about whether it implements snapshot isolation~\cite{galera-claim,galera-notclaim}, and AntidoteDB~\cite{antidote}, which claims to implement causal consistency~\cite{antidote-claim}.
%Galera~\cite{galera}, and AntidoteDB~\cite{antidote}, which claim to implement serializability~\cite{cockroach-claim}, snapshot isolation~\cite{galera-claim} and causal consistency~\cite{antidote-claim}, respectively (in the default configuration). 
Our implementation reports violations of these criteria in all cases. 
%In the case of CockroachDB, the documentation admits possible anomalies while in the case of Galera we confirm an open issue submitted on Github~\cite{galera-issue}. 
The consistency violations we found for AntidoteDB are novel and have been confirmed by its developers. We show that our algorithms are efficient and scalable.
%and they outperform an encoding to boolean satisfiability of the consistency models. 
In particular, we show that, although the asymptotic complexity of our algorithms is exponential in general (w.r.t. the number of sessions), the worst-case behavior is not exercised in practice.


%%%%%%
% \tool{}
%%%%%%

\section{Applications implemented on Distributed Datastores}

Modern applications require not just data
reliability, but also high-throughput concurrent accesses. 
Applications concerning supply chains, banking, etc. use traditional relational databases
for storing and processing data, whereas applications such as social networking
software and e-commerce platforms 
use cloud-based storage systems (such as Azure CosmosDb \cite{cosmosdb}, Amazon DynamoDb
\cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}, Facebook TAO \cite{DBLP:conf/usenix/BronsonACCDDFGKLMPPSV13}, etc.).


%As applications have moved from a single-box environment to the cloud, the notion of
%data persistence has also changed. It is no longer about storing data on a
%single disk with a single point of access. Rather, modern applications such as
%social networking software, e-commerce platforms, cloud micro-services, etc. are built using 
%high-scale storage systems, such as Azure CosmosDb \cite{cosmosdb}, Amazon DynamoDb \cite{DBLP:conf/sigmod/Sivasubramanian12}, 
%Facebook TAO \cite{DBLP:conf/usenix/BronsonACCDDFGKLMPPSV13}. Applications such as 
 
%These storage systems, commonly offered by most major cloud providers (such as
%Azure CosmosDb \cite{cosmosdb}, Amazon DynamoDb \cite{DBLP:conf/sigmod/Sivasubramanian12}, 
%Facebook TAO \cite{DBLP:conf/usenix/BronsonACCDDFGKLMPPSV13}, etc.)
%create and manage multiple replicas of data. Having multiple replicas offers reliability and prevents
%data loss, but it also offers availability and low-latency accesses by allowing
%different clients to connect with different replicas. 

Providing high-throughput processing, unfortunately, comes at an unavoidable cost of weakening 
the guarantees offered to users.
Concurrently-connected clients may end up observing different views of the same data. 
These ``anomalies'' can be prevented by using a strong \textit{isolation level} 
such as \textit{serializability}, which essentially offers a single view of the
data. However, serializability requires expensive synchronization and incurs a high performance cost. As a
consequence, most storage systems use weaker isolation levels, such as 
{\it Causal Consistency}~\cite{DBLP:journals/cacm/Lamport78,DBLP:conf/sosp/LloydFKA11,antidote-white-paper},
{\it Snapshot Isolation}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, {\it Read
Committed}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, etc. for better performance.
In a recent survey of
database administrators \cite{DBLP:conf/sigmod/Pavlo17}, 86\% of the participants responded that
most or all of the transactions in their databases execute at read committed isolation level.

A weaker isolation level allows for more possible behaviors than stronger
isolation levels. It is up to the developers then to ensure that their
application can tolerate this larger set of behaviors. Unfortunately, weak
isolation levels are hard to understand or reason about
\cite{DBLP:conf/popl/BrutschyD0V17,adya-thesis} and resulting application bugs
can cause loss of business \cite{DBLP:conf/sigmod/WarszawskiB17}.


\subsection{State of the Art}

This work addresses the problem of \textit{testing} code for correctness
against weak behaviors: a developer should be able to write a test that runs
their application and then asserts for correct behavior. 
The main difficulty today is getting coverage of weak behaviors during
the test. If one runs the test
against the actual production storage system, it is very likely to only result in
serializable behaviors because of their optimized implementation. For
instance, only 0.0004\% of all reads performed on Facebook's TAO storage system 
were not serializable \cite{DBLP:conf/sosp/LuVAHSTKL15}. 
Emulators, offered by cloud providers for local development, on the other hand, do not support weaker
isolation levels at all \cite{cosmosdb-local}. Another option, possible when
the storage system is available open-source, is to set it up with a 
tool like Jepsen~\cite{jepsen} to inject noise (bring down replicas or
delay packets on the network). 
This approach is unable to provide good coverage at the level of client operations
\cite{DBLP:journals/pacmpl/RahmaniNDJ19} (\sectref{oltp}). Another line of work has focussed on finding
anomalies by identifying non-serializable behavior (\sectref{app:related}). Anomalies, however, do not
always correspond to bugs \cite{DBLP:conf/pldi/BrutschyD0V18,DBLP:journals/pvldb/GanRRB020}; they may
either not be important (e.g., gather statistics) or may already be handled in
the application (e.g., checking and deleting duplicate items).

\subsection{Contribution}

%Prior work on this problem has largely focussed on
%formal verification techniques: to establish correctness of code against a specification 
%of a particular isolation level \cite{DBLP:journals/pacmpl/RahmaniNDJ19,DBLP:journals/sigsoft/DabaghchianROMT17} (\akash{others?}). Verification requires
%statically analyzing application code; with such an approach, in addition to
%scalability problems, it is often difficult to support
%various programming styles, libraries, frameworks and languages. 
%For these reasons, testing is still the more widely adopted engineering practice. 
%Our goal is support testing of any application, with little to no modifications.
%We defer more details to the related work section.


%
%There 
%is informal documentation available \cite{cosmosdb-consistency} as well as
%formal specifications \akash{cite CosmosDb-TLA}, but none of it is immediately 
%actionable for a developer. 


%Modern applications such as social networking, ecommerce, etc., use
%highly-available low-latency geo-replicated storage systems~\cite{cosmosdb} to
%achieve high performance and scalability.
%These storage systems must replicate data for persistence, and then allow
%clients to connect with different replicas for availability on failures and for
%low latency.
%The replicas communicate updates to each other in the background using message
%passing.
%However, unfortunately, maintaining strong consistency across these replicas
%requires global synchronization which incurs high performance overheads.
%Moreover, as stated by the Consistency, Availability, and Partition-tolerance
%(CAP) theorem~\cite{DBLP:journals/sigact/GilbertL02},
%it is not possible for such storage systems to remain available and
%simultaneously guarantee consistency under network partitions (which are
%unavoidable).
%Hence, to provide high availability and low latency, many distributed data
%stores provide only weak consistency guarantees, formally defined as different
%consistency models: {\it Causal Consistency}~\cite{DBLP:conf/popl/BouajjaniEGH17,DBLP:journals/cacm/Lamport78},
%{\it Snapshot Isolation}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, and {\it Read
%Committed}~\cite{DBLP:conf/sigmod/BerensonBGMOO95}, etc.
%This current scenario is also showcased in a recent Database Admin
%Survey~\cite{DBLP:conf/sigmod/Pavlo17} where more than 73\% participants responded that all the
%transactions in their databases execute at read committed consistency level.

%THE NEXT PARAGRAPH SHOULD BE ONLY ABOUT: PROGRAMMING ON TOP OF WEAK ISOLATION IS HARD. 

%The weak isolation semantics of these consistency models permit various
%anomalies that violate data consistency; for example, lost updates,
%non-repeatable reads etc. 
%(DIRTY READS IS NOT AN ANOMALY ABOVE READ COMMITTED).
%Such anomalies often lead to undesirable executions in client applications and manifest in the form of invariant violations (assertion violations).
%For example, consider an online store with a shopping cart
%service~\ref{fig:motiv}.
%If a user is accessing the cart from multiple clients, and deletes an item from
%one client. 
%Under weak consistency, not only that delete operation can take some time to be
%visible through another client, but even after viewing deletion, the item could
%appear again in the cart \cite{DBLP:conf/sosp/DeCandiaHJKLPSVV07}.
%%\textcolor{blue}{Shopping cart example showing assertion violation on weak consistency.}
%To prevent them, developers should be aware of such anomalies and use explicit
%synchronization at appropriate program points in their applications. 
%(TODO: LOCKING IS TOO SPECIFIC, USE SYNCHRONIZATION INSTEAD).
%This requirement makes application development extremely challenging, because
%such weak isolation semantics are hard to understand and
%reason~\cite{DBLP:conf/popl/BrutschyD0V17,adya-thesis}, compared to the simple case of
%serializability where one can argue about one transaction at a time. Further,
%often these consistency levels are informally explained with low-level
%implementation details, leading to poor understanding.
%For example, Cosmos DB~\cite{cosmosdb} defines five levels of consistency with
%only rough guidelines on which one to pick~\cite{cosmosdb-consistency}.
%(THE FLOW IS A LITTLE BIT AKWARD: WEAK ISOLATION IS HARD TO UNDERSTAND COMPARED TO SERIALIZABILITY; MENTION THE Cosmos DB STUFF AS SUPPORT => INSERTING THE RIGHT SYNCHRONIZATION IS HARD => APPLICATION DEVELOPMENT IS HARD. WHY IS TESTING HARD BECAUSE WEAK ISOLATION IS COMPLICATED ?) 

We present \tool{}, a mock in-memory storage system meant for testing
correctness of storage-backed applications. 
\tool{} supports 
common APIs for accessing data (key-value updates, as well as SQL queries),
making it an easy substitute for an actual storage system. \tool{}
can be configured with one of several isolation levels. 
%Currently,
%\tool{} supports Serializability, Causal Consistency as well as Read
%Committed. (Addition of other isolation levels is easy.)
% On a read operation, \tool{} computes the set of all possible return values
% allowed under the chosen isolation level, and randomly returns one of them. The
% developer can then simply execute their test multiple times to get coverage of
% possible weak behaviors. For the program in \figref{motiv}, if we write a test
% asserting that two sequential reads cannot return empty-cart followed by $\{I,
% I\}$, then it takes only 20 runs of the test (on average) to fail the
% assert. In contrast, the test does not fail when using MySQL with read committed, 
% even after 100k runs. 
%\tool{} can work with any application with little to no modifications:
%a developer simply needs to link their tests to \tool{}, instead of the
%production storage system.

\tool{} does not rely on stress generation, fault
injection, or data replication. 
Rather, it works directly with a formalization of
the given isolation level in order to compute allowed return values.
The theory behind \tool{} builds on the axiomatic definitions of isolation
levels introduced in chapter \ref{chap:txn} \cite{DBLP:journals/pacmpl/BiswasE19}. These
definitions use logical constraints (called \emph{axioms}) to characterize the
set of executions of a key-value store that conform to a particular isolation
level (we discuss SQL queries later).
% These constraints refer to a specific set of
% relations between events/transactions in an execution that describe control-flow
% or data-flow dependencies: a program order $\po$ between events in the same
% transaction, a session order $\so$ between transactions in the same session\footnote{A
% session is a sequential interface to the storage system. It corresponds to what
% is also called a connection.}, and a write-read $\wro$ (read-from) relation that
% associates each read event with a transaction that writes the value returned by
% the read. These relations along with the events (also called, operations) in an
% execution are called a \emph{history}. The history corresponding to the 
% shopping cart anomaly explained above is given on the bottom of Figure~\ref{fig:motiv}.
% Read operations include the read value, and boxes group events from the same transaction.
% %The initial value of the key is supposed to be written in a fictitious transaction. 
% A history describes only the
% interaction with the key-value store, omitting application side events (e.g., computing
% the value to be written to a key). 

% \tool{} implements a \emph{centralized} operational semantics for key-value stores, which is based on these axiomatic definitions. Transactions are executed \emph{serially}, one after another, the concurrency being simulated during the handling of read events.  
% This semantics maintains a history that contains all the past events (from all
% transactions/sessions), and write events are simply added to the history. The
% value returned by a read event is established based on a non-deterministic
% choice of a write-read dependency (concerning this read event) that satisfies
% the axioms of the considered isolation level.
% %(\tool{} resolves any non-determinism in a random fashion). 
% Depending on the weakness of the isolation
% level, this makes it possible to return values written in arbitrarily ``old''
% transactions, and simulate any concurrent behavior. For instance, the history in Figure~\ref{fig:motiv}
% can be obtained by executing \texttt{AddItem}, \texttt{DeleteItem}, and then the two reads (serially).
% The read in \texttt{DeleteItem} can take its value from the initial state and ``ignore'' the
% previously executed \texttt{AddItem}, because the obtained history validates the axioms of 
% causal consistency (or read committed). The same happens for the two later reads in the same
% session, the first one being able to read from \texttt{DeleteItem} and the second one
% from \texttt{AddItem}.

% We formally prove that this semantics does indeed simulate any concurrent behavior, by 
% showing that it is equivalent to a semantics where transactions are allowed to interleave.
% In comparison with concrete implementations, this semantics makes it possible to handle 
% a wide range of isolation levels in a uniform way. It only has two sources of
% non-determinism: 
% the order in which entire transactions are submitted, and the choice of write-read dependencies in read 
% events. This enable better coverage of possible behaviors, the penalty in performance not
% being an issue in safety testing workloads which are usually small (see our evaluation). 

We consider a set of micro-benchmarks Twitter \cite{twissandra}, Shopping Cart \cite{DBLP:conf/pldi/Sivaramakrishnan15}, Courseware \cite{DBLP:conf/esop/NairP020}, Treiber Stack \cite{DBLP:conf/cav/NagarMJ20} inspired from real-world applications 
(\sectref{micro-benchmarks}) and evaluate the number of test iterations
required to fail an invalid assertion 
(\sectref{micro-assertion-violations}). We also measure the \textit{coverage} of
weak behaviors provided by \tool{} (\sectref{micro-coverage}). Each of these
applications were implemented based on their specifications described in prior
work; they all use \tool{} as a library, via its KV interface.  


We also extend our semantics to cover SQL queries as well, by compiling SQL queries down to transactions with multiple key-value reads/writes. Naturally, we considered OLTPBench \cite{DBLP:journals/pvldb/DifallahPCC13}, a popular benchmark suite of representative 
OLTP workloads for relational databases. We picked a subset of OLTPBench, including TPC-C, for which we had reasonable assertions. We showed using our technique, \tool{} was able to violate these assertion under weak isolation levels.
% A table in a relational database is represented using a set of primary key values (identifying uniquely the set of rows) and a set of keys, one for each cell in the table. The set of primary key values is represented using a set of Boolean key-value pairs that simulate its characteristic function (adding or removing an element corresponds to updating one of these keys to $\btrue$ or $\bfalse$). Then, SQL queries are compiled to read or write accesses to the keys representing a table. For instance, a $\mathtt{SELECT}$ query that retrieves the set of rows in a table that satisfy a $\mathtt{WHERE}$ condition is compiled to (1) reading Boolean keys to identify the primary key values of the rows contained in the table, (2) reading keys that represent columns used in the $\mathtt{WHERE}$ condition, and (3) reading all the keys that represent cells in a row satisfying the $\mathtt{WHERE}$ condition. This rewriting contains the minimal set of accesses to the cells of a table that are needed to ensure the conventional specification of SQL.
% It makes it possible to ``export'' formalizations of key-value store isolation levels to SQL transactions.

%This paper first presents an axiomatic semantics for various isolation levels 
%in Key-Value stores that allows one to reason about the set of valid behaviors
%under a given isolation level. We follow this by a non-deterministic operation
%semantics, equivalent to the axiomatic semantics, where
%each operation is cooperatively scheduled to execute one at a time (serially).
%The operational semantics maintains a history of the read-write operations,
%including a \textit{read-from} relationship that matches reads to previous
%writes. On the submission of a new operation, this history is extended in
%accordance with this operational semantics. \tool{} implements the operational
%semantics, while resolving any non-determinism in a random fashion. 
%We also extend our semantics to cover SQL queries as well, by compiling SQL
%queries down to transactions with multiple key-value updates.

% \paragraph{Contributions}
%We implemented \tool{} to support an interface consistent with key-value
%stores and databases, which allows us to directly run real applications unmodified. 
%We evaluated \tool{} on a series of micro-benchmarks, inspired from real
%applications, as well as the well-known OLTPBench \cite{DBLP:journals/pvldb/DifallahPCC13}
%that is used for evaluating
%databases on OLTP workloads. 

% This paper makes the following contributions:
% \begin{itemize}
% \item We define an operational semantics for key-value stores under various
%   isolation levels, which simulates all concurrent behaviors with executions
%   where transactions execute serially (\sectref{op-kv}) and which is based 
%   on the axiomatic definitions in~\cite{DBLP:journals/pacmpl/BiswasE19} (and outlined in \S\ref{sec:def}),
% \item We broaden the scope of the key-value store semantics to SQL transactions
%   using a compiler that rewrites SQL queries to key-value accesses (\sectref{SQL-to-KV}),
% \item The operational semantics and the SQL compiler are implemented in a tool
%   called \tool{} (\sectref{impl}). It randomly resolves possible choices to provide coverage
%   of weak behaviors. It supports both a key-value interface as well as SQL,
%   making it readily compatible with any storage-backed application.
% \item We present an evaluation of \tool{} on several applications, showcasing
% its superior coverage of weak behaviors as well as bug-finding abilities
% (\sectref{micro}, \sectref{oltp}).\footnote{Source code of our benchmarks is available
% as supplementary material.}
% \end{itemize}