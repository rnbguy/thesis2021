%!TEX root = ../Thesis.tex

This dissertation introduces some previously unexplored open problems in the distributed domains. In general, many of the weak consistencies and data types are not formally characterized. The state of the art testing frameworks are using ad-hoc pattern matching techniques and network manipulation without guaranteeing proper testing and bug coverage. We provide formal characterizations for these systems and come up with algorithms which are able to provide sound and complete testing and provide better coverage.

In chapter \ref{chap:crdt}, we provide formal characterizations for conflict free replicated data types. The characterizations enable us to reason about the conformance of the histories of these data types corresponding to their weaker consistencies. We further explore the asymptotic complexities of the checking conformance of these histories. We prove intractability and tractability results for these data types.

Next, in chapter \ref{chap:txn}, we move on to transactional systems with reads and writes. We explore different levels of weak consistencies for transactional systems. We propose novel characterizations for these weak consistencies over transactional systems. Using these characterizations we study the asymptotic complexities of checking these consistencies for transactional histories. We prove intractability and tractability results for these weak consistencies. But finally, we show these intractable cases are tractable if we bound the number of sessions in the histories. We further extend this idea on a graph replicas where the bi-connected components are bounded.

Lastly in chapter \ref{chap:dist-app}, we explore if we can use our characterizations for transactional systems in chapter \ref{chap:txn} to reason about distributed applications built on top of some distributed systems. Usually these applications are benchmarked on real databases relying on ad-hoc network manipulation. Since these systems are highly optimized, it is very very hard to find the bugs due to corner cases. So benchmark based on these real databases do not give good coverage. We presented \tool{}, which models different weak consistency models using our previous characterizations. \tool{} simulates a consistency model keeping track of a global history and making uniform choices which satisfies the corresponding consistency criteria. This drastically improves the test coverage, and we imperically show \tool{} violates invariants in popular OLTP benchmarks in few attempts.

\section{Future Work}
\label{sec:global-future-work}

In chapter \ref{chap:crdt}, The general problem of checking admissible problems for Flags, Set still remains open. We provide a sound algorithm which runs in polynomial-time. Also the formal characterizations of CRDTs establish the necessary framework of a similar work that we have done in chapter \ref{chap:dist-app}.

Concerning chapter \ref{chap:txn}, we provide a complete framework to test transactional systems. The possible next work would be find root causes for inconsistent history. Sometimes, the bugs are not visible in small number of sessions. Our tests on AntidoteDB exposed a bug when the number of sessions is 42 which is confirmed by their developers. When the number of history is too big, it is natural to ask the smallest sub-history which violates the consistency to pin point the point of failure.

Next we can try to extend our \emph{offline} algorithm to an \emph{online} one. For now, we record a history of a system at first, then run our algorithm. This is enough for unit test suits. But in production, someone would want to run an \emph{auditor} like tool to make sure the consistency of these systems are always maintained. Also, when an \emph{inconsistency} is detected how to pin point to the point of failure and recover from it.

Running the testing algorithm online, pin-pointing to point of failure and recovering - these seems very challenging considering, in the real time system, one has to think about overheads. Finding such algorithm with low resource footprint for small overheads still remains challenging.

Next in the context of \ref{chap:dist-app}, one of the problem that developers come across often is to write good sets of invariants for unit tests. It would be interesting to see, provided a distributed application and the consistency model required, can we generate a set of invariants for that application which is enough to capture the consistency bugs under some weaker consistency models?

It would also be an interesting problem to find the weakest possible consistency model for an application where it still runs correctly \ie the weaker consistency model does not introduce new undesirable behaviors. This would help the developers to choose the best consistency levels for their applications, yet boost the concurrency of their applications.

In some modern databases, each operation is performed at different level of consistency models. It would be interesting to study the formal characterization problem of these hybrid-consistent systems and provide tractable conformance checking algorithms.

Finally, \tool{} still remains a prototype of our work. Although it is made to work on OLTPBenchmark, a lot of engineering work is left to be done to make \tool{} work with the modern real world applications. We want to to provide a complete standalone database which can be used to test a larger set of real world applications. 