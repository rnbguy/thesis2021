%!TEX root = ../Thesis.tex

In this thesis, we have investigated various algorithmic questions related to automated testing of weakly-consistent data storage systems and applications built on top of them. We have explored the issue of specifying such systems, and studied the theoretical limits of checking whether a given execution satisfies the intended specification.
%introduced specification formalisms for data type abstractions like CRDTs or transactional systems (databases), and studied the theoretical limits of checking whether a given execution satisfies the intended specification. 
 The contributions of this thesis span several directions: (1) new formalisms for specifying weakly-consistent behaviors which integrate data type abstractions like counters, registers, sets, lists, etc, or transactions with various degrees of isolation, (2) new asymptotic complexity results that delineate the tractability of automated testing for data storage systems, and (3) an effective methodology for improving the test coverage of storage-backed applications.

% introduces some previously unexplored open problems in the distributed domains. In general, many of the weak consistencies and data types are not formally characterized. The state of the art testing frameworks are using ad-hoc pattern matching techniques and network manipulation without guaranteeing proper testing and bug coverage. We provide formal characterizations for these systems and come up with algorithms which are able to provide sound and complete testing and provide better coverage.

In more detail, Chapter~\ref{chap:crdt} focused on CRDTs, an important class of replicated data types that offers a suitable compromise between consistency and availability. We have introduced a new specification formalism that provides a seamless integration between a particular data type semantics and consistency properties related to the asynchronous propagation of updates. We have used this formalism to derive new complexity results concerning the problem of checking conformance for a given execution. 
%  we provide formal characterizations for conflict free replicated data types. The characterizations enable us to reason about the conformance of the histories of these data types corresponding to their weaker consistencies. We further explore the asymptotic complexities of the checking conformance of these histories. We prove intractability and tractability results for these data types.

Chapter~\ref{chap:txn} investigated the same issues, but in the case of transactional key-value stores. We propose new definitions for established consistency models, which compared to previous approaches, are expressed by logical constraints that follow a common template and make it possible to better distill semantical differences. We have also established interesting semantical relationships between weak consistency models like Prefix Consistency or Snapshot Isolation, and Serializability. These advancements were used to ultimately derive complexity results about checking correctness of transactional key-value store executions, and determine the limits of tractability. 

% we move on to transactional systems with reads and writes. We explore different levels of weak consistencies for transactional systems. We propose novel characterizations for these weak consistencies over transactional systems. Using these characterizations we study the asymptotic complexities of checking these consistencies for transactional histories. We prove intractability and tractability results for these weak consistencies. But finally, we show these intractable cases are tractable if we bound the number of sessions in the histories. We further extend this idea on a graph replicas where the bi-connected components are bounded.

Chapter~\ref{chap:dist-app} uses the specification formalism presented in Chapter~\ref{chap:txn} in order to design a mock in-memory storage system called \tool{} that makes it possible to improve coverage in testing applications built on top of transactional storage systems. \tool{} simulates the behaviors of a storage system satisfying a specific consistency model by keeping a global history of previously executed operations and making uniform random choices on read operations. Our empirical evaluation shows that \tool{} makes it possible to uncover invariant violations in established OLTP benchmarks in a small number of attempts.

%Lastly in chapter \ref{chap:dist-app}, we explore if we can use our characterizations for transactional systems in chapter \ref{chap:txn} to reason about distributed applications built on top of some distributed systems. Usually these applications are benchmarked on real databases relying on ad-hoc network manipulation. Since these systems are highly optimized, it is very very hard to find the bugs due to corner cases. So benchmark based on these real databases do not give good coverage. We presented \tool{}, which models different weak consistency models using our previous characterizations. \tool{} simulates a consistency model keeping track of a global history and making uniform choices which satisfies the corresponding consistency criteria. This drastically improves the test coverage, and we imperically show \tool{} violates invariants in popular OLTP benchmarks in few attempts.

\section{Future Work}
\label{sec:global-future-work}

The work in this thesis can be advanced along several directions:
\begin{itemize}
\item Chapter~\ref{chap:crdt} leaves open several questions related to the complexity of CRDT consistency checking: checking conformance to the counter CRDT when the number of replicas is bounded, or sets and flags when their sizes are also bounded. We believe that these problems remain polynomial time, but as we explained in that chapter, the algorithms introduced in our previous work~\cite{DBLP:conf/cav/BiswasEE19} are only sound.

% The general problem of checking admissible problems for Flags, Set still remains open. We provide a sound algorithm which runs in polynomial-time. Also the formal characterizations of CRDTs establish the necessary framework of a similar work that we have done in chapter \ref{chap:dist-app}.

\item Our conformance checking algorithms are \emph{offline}, in the sense, that they receive as input an entire execution. For future work, we want to explore \emph{online} algorithms that process a given execution on the fly. Designing such algorithms with a low resource footprint or small overhead is a highly challenging issue.

%Next we can try to extend our \emph{offline} algorithm to an \emph{online} one. For now, we record a history of a system at first, then run our algorithm. This is enough for unit test suits. But in production, someone would want to run an \emph{auditor} like tool to make sure the consistency of these systems are always maintained. Also, when an \emph{inconsistency} is detected how to pin point to the point of failure and recover from it.
%
%Running the testing algorithm online, pin-pointing to point of failure and recovering - these seems very challenging considering, in the real time system, one has to think about overheads. Finding such algorithm with low resource footprint for small overheads still remains challenging.

\item While our algorithms can only be used to indicate whether an execution is correct or not, we would like to investigate the issue of \emph{root-causing} violations. Some bugs are difficult to expose with small length executions. For instance, our tests on AntidoteDB exposed a bug in an execution with 42 transactions, which has been confirmed by the developers, and which cannot be caught with smaller executions (up to our knowledge). In such cases, pin pointing the root cause becomes essential for developers being able to repair it.

%  the number of sessions is 42 which is confirmed by their developers. When the number of history is too big, it is natural to ask the smallest sub-history which violates the consistency to pin point the point of failure.
%

\item Modern data storage systems support operations/transactions at different levels of consistency. While our work has assumed that all operations/transactions behave under the same consistency model, extending it to such cases is an important research direction.

\item Concerning the problem of testing applications, a frequent issue is the lack of precise specifications when checking their correctness against a weak consistency model. An interesting direction for future work is trying to automatically synthesize application-level invariants that distinguish its behaviors under strong consistency versus weak consistency. These invariants could be used during the development process as a way of guiding the insertion of additional synchronization. 

%Next in the context of \ref{chap:dist-app}, one of the problem that developers come across often is to write good sets of invariants for unit tests. It would be interesting to see, provided a distributed application and the consistency model required, can we generate a set of invariants for that application which is enough to capture the consistency bugs under some weaker consistency models?

\item More generally, an important issue is finding the weakest possible consistency model for which an application satisfies the intended specification. This would help in improving the performance of a given application, since weaker consistency models boost concurrency and minimize the synchronization overhead.

%the developers to choose the best consistency levels for their applications, yet boost the concurrency of their applications.

%In some modern databases, each operation is performed at different level of consistency models. It would be interesting to study the formal characterization problem of these hybrid-consistent systems and provide tractable conformance checking algorithms.

%Finally, \tool{} still remains a prototype of our work. Although it is made to work on OLTPBenchmark, a lot of engineering work is left to be done to make \tool{} work with the modern real world applications. We want to to provide a complete standalone database which can be used to test a larger set of real world applications. 
\end{itemize}
